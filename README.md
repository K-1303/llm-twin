# LLM Twin - Content Generation based on your Unique styling using LLMs

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Architecture](#-architecture)
- [System Components](#-system-components)
- [Project Structure](#-project-structure)
- [Installation](#-installation)

---

## ğŸ¯ Overview

**LLM Twin** is a comprehensive LLMOps system that creates an AI-powered "digital twin" by:

1. **Crawling** digital content (Medium articles, LinkedIn posts, GitHub repositories, Twitter/X posts)
2. **Processing** and cleaning the data through feature engineering pipelines
3. **Embedding** content into vector representations for semantic search
4. **Retrieving** relevant context using advanced RAG techniques
5. **Generating** personalized content using fine-tuned LLMs/Gemini

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Ingestion Layer                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Crawlers: Medium â”‚ LinkedIn â”‚ GitHub â”‚ Twitter/X â”‚ Custom      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Storage Layer (MongoDB)                     â”‚
â”‚         Raw Documents: Posts â”‚ Articles â”‚ Repositories          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Feature Engineering Pipeline                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Data Cleaning    â”‚  2. Chunking    â”‚  3. Embedding          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Vector Storage (Qdrant)                       â”‚
â”‚         Embedded Chunks with Semantic Search Index               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RAG Retrieval System                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Query Expansion â”‚ Self-Query â”‚ Semantic Search â”‚ Reranking     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LLM Inference (Gemini)                        â”‚
â”‚              Content Generation with Context                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© System Components

### 1. **Data Crawlers** (`llm_engineering/application/crawlers/`)

Specialized crawlers for different platforms:

- **MediumCrawler**: Extracts articles using Selenium
- **LinkedInCrawler**: Scrapes posts and profiles (deprecated due to LinkedIn changes)
- **GithubCrawler**: Clones repositories and extracts code
- **TwitterCrawler**: Fetches tweets via ScrapingDog API
- **CustomArticleCrawler**: Generic HTML content extraction

### 2. **Data Storage**

**MongoDB (NoSQL)**
- Stores raw documents with metadata
- Collections: `posts`, `articles`, `repositories`, `users`

**Qdrant (Vector DB)**
- Stores embedded chunks for similarity search
- Collections: `embedded_posts`, `embedded_articles`, `embedded_repositories`

### 3. **Feature Engineering** (`llm_engineering/application/preprocessing/`)

**Cleaning Pipeline**
- Removes special characters and normalizes text
- Extracts meaningful content from structured data

**Chunking Pipeline**
- Posts: 250 tokens (overlap: 25)
- Articles: 1000-2000 characters (semantic chunking)
- Repositories: 1500 tokens (overlap: 100)

**Embedding Pipeline**
- Model: `sentence-transformers/all-MiniLM-L6-v2`
- Generates 384-dimensional embeddings
- Batch processing for efficiency

### 4. **RAG System** (`llm_engineering/application/rag/`)

**Query Expansion**
- Generates multiple query variations using Gemini
- Improves recall by exploring different perspectives

**Self-Query**
- Extracts author information from queries
- Filters results by user context

**Retrieval**
- Parallel search across data categories
- Top-k selection with diversity

**Reranking**
- Cross-encoder model: `cross-encoder/ms-marco-MiniLM-L-4-v2`
- Refines results based on query-document relevance

### 5. **LLM Inference** (`llm_engineering/model/inference/`)

- **Model**: Google Gemini 2.0 Flash
- **Parameters**: Temperature=0.0, Max tokens=256
- **Prompt Engineering**: Context-aware generation
- **Monitoring**: Opik/Comet ML integration

---

## ğŸ“ Project Structure

```
llm-twin/
â”œâ”€â”€ llm_engineering/              # Core application code
â”‚   â”œâ”€â”€ application/               # Application layer
â”‚   â”‚   â”œâ”€â”€ crawlers/              # Data extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py            # Abstract crawler classes
â”‚   â”‚   â”‚   â”œâ”€â”€ dispatcher.py     # Crawler routing
â”‚   â”‚   â”‚   â”œâ”€â”€ medium.py          # Medium scraper
â”‚   â”‚   â”‚   â”œâ”€â”€ linkedin.py        # LinkedIn scraper
â”‚   â”‚   â”‚   â”œâ”€â”€ github.py          # GitHub scraper
â”‚   â”‚   â”‚   â”œâ”€â”€ twitter.py         # Twitter/X scraper
â”‚   â”‚   â”‚   â””â”€â”€ custom_article.py  # Generic article scraper
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ networks/              # ML models
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py            # Singleton pattern
â”‚   â”‚   â”‚   â””â”€â”€ embeddings.py      # Embedding & reranking models
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ preprocessing/         # Data transformation
â”‚   â”‚   â”‚   â”œâ”€â”€ cleaning_data_handlers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chunking_data_handlers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding_data_handlers.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dispatchers.py     # Processing orchestration
â”‚   â”‚   â”‚   â””â”€â”€ operations/        # Core operations
â”‚   â”‚   â”‚       â”œâ”€â”€ cleaning.py
â”‚   â”‚   â”‚       â””â”€â”€ chunking.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ rag/                   # Retrieval system
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query_expansion.py
â”‚   â”‚   â”‚   â”œâ”€â”€ self_query.py
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py       # Main retrieval logic
â”‚   â”‚   â”‚   â”œâ”€â”€ reranking.py
â”‚   â”‚   â”‚   â””â”€â”€ prompt_templates.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ utils/                 # Utilities
â”‚   â”‚       â”œâ”€â”€ misc.py
â”‚   â”‚       â””â”€â”€ split_user_full_name.py
â”‚   â”‚
â”‚   â”œâ”€â”€ domain/                    # Domain models
â”‚   â”‚   â”œâ”€â”€ base/                  # Base classes
â”‚   â”‚   â”‚   â”œâ”€â”€ nosql.py           # MongoDB ODM
â”‚   â”‚   â”‚   â””â”€â”€ vector.py          # Qdrant ODM
â”‚   â”‚   â”œâ”€â”€ documents.py           # Raw document models
â”‚   â”‚   â”œâ”€â”€ cleaned_documents.py   # Cleaned data models
â”‚   â”‚   â”œâ”€â”€ chunks.py              # Chunk models
â”‚   â”‚   â”œâ”€â”€ embedded_chunks.py     # Embedded chunk models
â”‚   â”‚   â”œâ”€â”€ queries.py             # Query models
â”‚   â”‚   â”œâ”€â”€ types.py               # Enums & types
â”‚   â”‚   â””â”€â”€ exceptions.py          # Custom exceptions
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/            # External integrations
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”‚   â”œâ”€â”€ mongo.py           # MongoDB connection
â”‚   â”‚   â”‚   â””â”€â”€ qdrant.py          # Qdrant connection
â”‚   â”‚   â”œâ”€â”€ opik_utils.py          # Monitoring setup
â”‚   â”‚   â””â”€â”€ inference_pipeline_api.py  # FastAPI service
â”‚   â”‚
â”‚   â”œâ”€â”€ model/                     # LLM inference
â”‚   â”‚   â””â”€â”€ inference/
â”‚   â”‚       â”œâ”€â”€ inference.py       # Gemini integration
â”‚   â”‚       â”œâ”€â”€ run.py             # Execution logic
â”‚   â”‚       â””â”€â”€ test.py            # Manual testing
â”‚   â”‚
â”‚   â””â”€â”€ settings.py                # Configuration management
â”‚
â”œâ”€â”€ pipelines/                     # ZenML pipelines
â”‚   â”œâ”€â”€ digital_data_etl.py        # Data ingestion pipeline
â”‚   â””â”€â”€ feature_engineering.py     # Processing pipeline
â”‚
â”œâ”€â”€ steps/                         # Pipeline steps
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ get_or_create_user.py
â”‚   â”‚   â””â”€â”€ crawl_links.py
â”‚   â””â”€â”€ feature_engineering/
â”‚       â”œâ”€â”€ query_data_warehouse.py
â”‚       â”œâ”€â”€ clean.py
â”‚       â”œâ”€â”€ rag.py                 # Chunk & embed
â”‚       â””â”€â”€ load_to_vector_db.py
â”‚
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ run.py                     # CLI entry point
â”‚
â”œâ”€â”€ configs/                       # Pipeline configurations
â”‚   â”œâ”€â”€ digital_data_etl_*.yaml
â”‚   â””â”€â”€ feature_engineering.yaml
â”‚
â”œâ”€â”€ pyproject.toml                 # Dependencies & tasks
â”œâ”€â”€ .python-version                # Python version (3.11)
â””â”€â”€ README.md                      # This file
```

---

## ğŸš€ Installation

### Prerequisites

- Python 3.11
- [uv](https://github.com/astral-sh/uv) package manager
- MongoDB instance (local or Atlas)
- Qdrant instance (local or cloud)
- Google API key (Gemini)
- Optional: ScrapingDog API key (for Twitter), Comet ML (monitoring)

### Setup Steps

1. **Install uv package manager** (if not already installed):
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

2. **Clone the repository**:
```bash
git clone https://github.com/yourusername/llm-twin.git
cd llm-twin
```

3. **Install dependencies**:
```bash
uv sync
```

This reads `pyproject.toml` and installs all required packages in a virtual environment.

4. **Set up environment variables**:
```bash
cp .env.example .env
# Edit .env with your configuration
```

5. **Start required services**:

**MongoDB** (Docker):
```bash
docker run -d -p 27017:27017 --name mongodb mongo:latest
```

**Qdrant** (Docker):
```bash
docker run -d -p 6333:6333 --name qdrant qdrant/qdrant:latest
```

**ZenML** (optional, for UI):
```bash
uv run zenml up
```

---

## âš™ï¸ Configuration

### Environment Variables (`.env`)

```bash
# Database
DATABASE_NAME=llm-twin
DATABASE_HOST=mongodb://localhost:27017

# Qdrant
USE_QDRANT_CLOUD=false
QDRANT_DATABASE_HOST=localhost
QDRANT_DATABASE_PORT=6333
# For cloud:
# USE_QDRANT_CLOUD=true
# QDRANT_CLOUD_URL=https://your-cluster.qdrant.io
# QDRANT_APIKEY=your-api-key

# Google Gemini
GOOGLE_API_KEY=your-google-api-key
GOOGLE_GEMINI_MODEL=gemini-2.0-flash

# Models
TEXT_EMBEDDING_MODEL_ID=sentence-transformers/all-MiniLM-L6-v2
RERANKING_CROSS_ENCODER_MODEL_ID=cross-encoder/ms-marco-MiniLM-L-4-v2
RAG_MODEL_DEVICE=cpu

# Inference
TEMPERATURE_INFERENCE=0.0
MAX_NEW_TOKENS_INFERENCE=256
TOP_P_INFERENCE=0.9

# Optional: Twitter scraping
SCRAPINGDOG_API_KEY=your-scrapingdog-key

# Optional: Monitoring
COMET_API_KEY=your-comet-key
COMET_PROJECT=llm-twin

# Optional: HuggingFace
HF_TOKEN=your-hf-token
HF_USERNAME=your-username
```

### Pipeline Configurations (`configs/`)

**ETL Config Example** (`digital_data_etl_paul_iusztin.yaml`):
```yaml
parameters:
  user_full_name: "Paul Iusztin"
  links:
    - "https://medium.com/@pauliusztin/article-slug"
    - "https://github.com/username/repo"
    - "https://x.com/username/status/123456789"
```

**Feature Engineering Config** (`feature_engineering.yaml`):
```yaml
parameters:
  author_full_names:
    - "Paul Iusztin"
    - "Maxime Labonne"
```

---

## ğŸ’» Usage

### 1. Data Ingestion (ETL Pipeline)

Extract content from digital platforms:

```bash
# Run ETL for a specific user
uv run poe run-digital-data-etl-paul

# Or run for multiple users
uv run poe run-digital-data-etl

# With custom config
uv run python -m tools.run --run-etl --etl-config-filename your_config.yaml
```

### 2. Feature Engineering Pipeline

Process raw data into embeddings:

```bash
# Run feature engineering
uv run poe run-feature-engineering-pipeline

# Or directly
uv run python -m tools.run --run-feature-engineering
```


### 3. API Service (FastAPI)

Start the inference API:

```bash
uv run uvicorn llm_engineering.infrastructure.inference_pipeline_api:app --reload --port 8000
```

**Query the API:**

```bash
curl -X POST "http://localhost:8000/rag" \
  -H "Content-Type: application/json" \
  -d '{"query": "Explain vector databases"}'
```

**Response:**
```json
{
  "answer": "Vector databases are specialized systems designed for..."
}
```

